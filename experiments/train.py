"""
ä¸“åˆ©å®Œæ•´è®­ç»ƒæµç¨‹
"""
import sys
import os
from pathlib import Path
current_file = os.path.abspath(__file__)           # train.py çš„ç»å¯¹è·¯å¾„
experiments_dir = os.path.dirname(current_file)    # experiments/
project_root = os.path.dirname(experiments_dir)    # watermark/

# æ·»åŠ åˆ°æœç´¢è·¯å¾„
sys.path.insert(0, project_root)

print(f"ğŸ“ é¡¹ç›®æ ¹ç›®å½•: {project_root}")
# import sys
# sys.path.append('..')

import torch
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm

from models.encoder import Encoder
from models.decoder import Decoder
from models.attacks import HeterogeneousAttack
from models.sync_net import SyncNet
from utils.sync_pattern import SyncPatternGenerator
from utils.losses import CompositeLoss
from utils.dataset import get_dataloader
from utils.watermark_utils import WatermarkPreprocessor

class WatermarkTrainer:
    def __init__(self, config):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        """
        self.config = config
        
        # è®¾å¤‡
        self.device = torch.device(
            "mps" if torch.backends.mps.is_available() else "cpu"
        )
        print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # æ¨¡å‹
        self.encoder = Encoder(config['watermark_length']).to(self.device)
        self.decoder = Decoder(config['watermark_length']).to(self.device)
        self.attack = HeterogeneousAttack().to(self.device)
        
        # åŒæ­¥æ¨¡æ¿ç”Ÿæˆå™¨
        self.sync_generator = SyncPatternGenerator(config['image_size'])
        # åŒæ­¥ç½‘ç»œ
        self.sync_net = SyncNet().to(self.device)
        
        # æŸå¤±å‡½æ•°ï¼ˆLPIPSå¯èƒ½éœ€è¦CPUï¼‰
        self.criterion = CompositeLoss(
            lambda_p=config['lambda_p'],
            lambda_w=config['lambda_w']
        )

        # é¢„å¤„ç†å™¨ï¼ˆèº«ä»½+æ—¶é—´æˆ³+çº é”™+ç­¾å -> æ¯”ç‰¹è½½è·ï¼‰ï¼Œä½¿ç”¨æŒä¹…åŒ–å¯†é’¥
        key_dir = Path(config['save_dir']).parent / "keys"
        key_dir.mkdir(parents=True, exist_ok=True)
        private_key_path = key_dir / "private.pem"
        public_key_path = key_dir / "public.pem"
        self.preprocessor = WatermarkPreprocessor(
            private_key_path=str(private_key_path),
            public_key_path=str(public_key_path),
            target_bit_len=config['watermark_length'],
        )
        # é¦–æ¬¡ç”Ÿæˆæ—¶ä¿å­˜å¯†é’¥ï¼Œä¾¿äºæå–ç«¯éªŒç­¾
        if not private_key_path.exists() or not public_key_path.exists():
            self.preprocessor.save_keys(private_key_path, public_key_path)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(
            list(self.encoder.parameters()) + 
            list(self.decoder.parameters()),
            lr=config['lr']
        )
        # åŒæ­¥ç½‘ç»œä¼˜åŒ–å™¨
        self.sync_optimizer = optim.Adam(
            self.sync_net.parameters(),
            lr=config.get('sync_lr', config['lr'])
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦
        self.scheduler = optim.lr_scheduler.StepLR(
            self.optimizer,
            step_size=config['lr_step'],
            gamma=0.5
        )
        self.sync_scheduler = optim.lr_scheduler.StepLR(
            self.sync_optimizer,
            step_size=config['lr_step'],
            gamma=0.5
        )
        
        # TensorBoard
        self.writer = SummaryWriter(config['log_dir'])
        
        # æ•°æ®åŠ è½½å™¨
        self.train_loader = get_dataloader(
            config['train_dir'],
            batch_size=config['batch_size'],
            num_workers=config['num_workers'],
            preprocessor=self.preprocessor,
            pin_memory=config.get('pin_memory', True),
            watermark_length=config['watermark_length']
        )
    
    def train_epoch(self, epoch):
        """
        è®­ç»ƒä¸€ä¸ªepoch
        """
        self.encoder.train()
        self.decoder.train()
        self.sync_net.train()
        
        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch}')
        epoch_losses = {'total': 0, 'perceptual': 0, 'watermark': 0}
        
        for batch_idx, batch in enumerate(pbar):
            # æ•°æ®ç§»åˆ°è®¾å¤‡
            images = batch['image'].to(self.device)
            watermarks = batch['watermark'].to(self.device)
            
            # ç”ŸæˆåŒæ­¥æ¨¡æ¿
            sync_patterns = torch.stack([
                self.sync_generator.generate()
                for _ in range(images.size(0))
            ]).unsqueeze(1).to(self.device)

            # ===== åŒæ­¥ç½‘ç»œè®­ç»ƒï¼ˆéšæœºä»¿å°„ -> å›å½’é€†å˜æ¢ï¼‰=====
            # éšæœºä»¿å°„å‚æ•°
            angle = (torch.rand(images.size(0)) * 30 - 15).to(self.device)  # [-15,15]åº¦
            scale = (torch.rand(images.size(0)) * 0.4 + 0.8).to(self.device)  # [0.8,1.2]
            tx = (torch.rand(images.size(0)) * 0.1 - 0.05).to(self.device)    # [-0.05,0.05] å½’ä¸€åŒ–å¹³ç§»
            ty = (torch.rand(images.size(0)) * 0.1 - 0.05).to(self.device)
            rad = angle * torch.pi / 180
            cos_a = torch.cos(rad) * scale
            sin_a = torch.sin(rad) * scale
            theta_true = torch.zeros(images.size(0), 2, 3, device=self.device)
            theta_true[:,0,0] = cos_a; theta_true[:,0,1] = -sin_a; theta_true[:,0,2] = tx
            theta_true[:,1,0] = sin_a; theta_true[:,1,1] =  cos_a; theta_true[:,1,2] = ty

            grid = torch.nn.functional.affine_grid(theta_true, sync_patterns.size(), align_corners=False)
            warped = torch.nn.functional.grid_sample(sync_patterns, grid, align_corners=False)

            pred_theta = self.sync_net(warped)
            sync_loss = torch.nn.functional.mse_loss(pred_theta, theta_true)
            self.sync_optimizer.zero_grad()
            sync_loss.backward()
            self.sync_optimizer.step()
            
            # å‰å‘ä¼ æ’­
            # S2: åµŒå…¥æ°´å°
            watermarked = self.encoder(images, watermarks, sync_patterns)
            
            # S3: æ¨¡æ‹Ÿæ”»å‡»
            attacked = self.attack(watermarked)
            
            # S4: æå–æ°´å°
            pred_watermarks = self.decoder(attacked)
            
            # è®¡ç®—æŸå¤±
            # æ³¨æ„ï¼šLPIPSéœ€è¦CPU
            losses = self.criterion(
                watermarked.cpu(),
                images.cpu(),
                pred_watermarks,
                watermarks
            )
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            losses['total'].backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(
                list(self.encoder.parameters()) + 
                list(self.decoder.parameters()),
                max_norm=1.0
            )
            
            self.optimizer.step()
            
            # ç´¯è®¡æŸå¤±
            for k in epoch_losses:
                epoch_losses[k] += losses[k].item()
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': f"{losses['total'].item():.4f}",
                'p_loss': f"{losses['perceptual'].item():.4f}",
                'w_loss': f"{losses['watermark'].item():.4f}"
            })
            
            # TensorBoardè®°å½•
            global_step = epoch * len(self.train_loader) + batch_idx
            for k, v in losses.items():
                self.writer.add_scalar(f'train/{k}', v.item(), global_step)
        
        # å¹³å‡æŸå¤±
        for k in epoch_losses:
            epoch_losses[k] /= len(self.train_loader)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        self.sync_scheduler.step()
        return epoch_losses
    
    def train(self):
        """
        å®Œæ•´è®­ç»ƒæµç¨‹
        """
        print(f"\n{'='*50}")
        print(f"ğŸ¯ å¼€å§‹è®­ç»ƒæ°´å°ç³»ç»Ÿ")
        print(f"{'='*50}\n")
        
        best_loss = float('inf')
        
        for epoch in range(1, self.config['epochs'] + 1):
            # è®­ç»ƒ
            losses = self.train_epoch(epoch)
            
            # å­¦ä¹ ç‡è°ƒæ•´
            self.scheduler.step()
            
            # æ‰“å°
            print(f"\nEpoch {epoch}/{self.config['epochs']}:")
            print(f"  Total Loss: {losses['total']:.4f}")
            print(f"  Perceptual: {losses['perceptual']:.4f}")
            print(f"  Watermark:  {losses['watermark']:.4f}")
            print(f"  LR: {self.optimizer.param_groups[0]['lr']:.6f}")
            
            # ä¿å­˜æ¨¡å‹
            if losses['total'] < best_loss:
                best_loss = losses['total']
                self.save_checkpoint(epoch, 'best')
                print(f"  âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (loss={best_loss:.4f})")
            
            # å®šæœŸä¿å­˜
            if epoch % self.config['save_interval'] == 0:
                self.save_checkpoint(epoch, f'epoch_{epoch}')
        
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
        self.writer.close()
    
    def save_checkpoint(self, epoch, name):
        """
        ä¿å­˜æ¨¡å‹
        """
        os.makedirs(self.config['save_dir'], exist_ok=True)
        
        checkpoint = {
            'epoch': epoch,
            'encoder': self.encoder.state_dict(),
            'decoder': self.decoder.state_dict(),
            'sync_net': self.sync_net.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'sync_optimizer': self.sync_optimizer.state_dict(),
            'config': self.config
        }
        
        path = os.path.join(self.config['save_dir'], f'{name}.pth')
        torch.save(checkpoint, path)


if __name__ == "__main__":
    # è®­ç»ƒé…ç½®
    data_dir = os.path.join(project_root, 'data/train_images')
    save_dir = os.path.join(project_root, 'results/checkpoints')
    log_dir = os.path.join(project_root, 'results/logs')
    config = {
        # æ•°æ®
        'train_dir': data_dir,  # éœ€è¦å‡†å¤‡å›¾åƒ
        'image_size': 256,
        'batch_size': 8,  # M4å¯ä»¥å¼€åˆ°16-32
        'num_workers': 0,  # MPS ä¸‹é¿å…å¤šè¿›ç¨‹ pickle bch å¯¹è±¡ï¼›éœ€å¹¶å‘å¯æ”¹>0å¹¶é‡æ„é¢„å¤„ç†å®ä¾‹åŒ–æ–¹å¼
        
        # æ¨¡å‹
        'watermark_length': 640,  # 64bitåŸæ–‡ + BCH(127,64,10) + 512bitç­¾å
        
        # è®­ç»ƒ
        'epochs': 10,
        'lr': 0.0001,
        'lr_step': 20,
        'lambda_p': 1.0,
        'lambda_w': 10.0,
        
        # ä¿å­˜
        'save_dir': save_dir,
        'log_dir': log_dir,
        'save_interval': 10
    }
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = WatermarkTrainer(config)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()