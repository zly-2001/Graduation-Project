"""
ä¸“åˆ©æ­¥éª¤S3ï¼šå¼‚æ„å¯å¾®æ”»å‡»æ¨¡å—
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import random
from torchvision import transforms

class HeterogeneousAttack(nn.Module):
    def __init__(self, use_light_aigc: bool = True, use_inpaint: bool = False, use_ip2p: bool = False, use_ddim: bool = True, no_attack: bool = False):
        """
        å¼‚æ„æ”»å‡»æ¨¡å—ï¼šæ¨¡æ‹Ÿå¤šç§æ”»å‡»
        
        Args:
            use_light_aigc: æ˜¯å¦ä½¿ç”¨è½»é‡AIGCæ”»å‡»ï¼ˆæ¨¡æ‹ŸDDIMï¼Œä½¿ç”¨ç›¸åŒå…¬å¼ä½†è½»é‡å®ç°ï¼‰
            use_inpaint: æ˜¯å¦ä½¿ç”¨inpaintingæ”»å‡»
            use_ip2p: æ˜¯å¦ä½¿ç”¨InstructPix2Pixæ”»å‡»
            use_ddim: æ˜¯å¦ä½¿ç”¨çœŸæ­£çš„DDIMæ”»å‡»ï¼ˆå®æ–½æ–¹å¼è¦æ±‚ï¼‰
            no_attack: æ˜¯å¦ç¦ç”¨æ‰€æœ‰æ”»å‡»ï¼ˆç¬¬ä¸€é˜¶æ®µè®­ç»ƒï¼šè®©æ¨¡å‹å…ˆå­¦ä¼šåŸºç¡€åµŒå…¥ï¼‰
        """
        super().__init__()
        
        # æ”»å‡»æ± æƒé‡ï¼ˆå¯å­¦ä¹ ï¼‰
        self.attack_weights = nn.Parameter(torch.ones(3))

        # è½®è¯¢æŒ‡é’ˆï¼Œç¡®ä¿æ¯ä¸ªæ”»å‡»åœ¨è‹¥å¹² batch å†…éƒ½è¢«ç”¨åˆ°
        self._attack_index = 0
        self.use_light_aigc = use_light_aigc
        self.use_inpaint = use_inpaint
        self.use_ip2p = use_ip2p
        self.use_ddim = use_ddim
        self.no_attack = no_attack  # æ— æ”»å‡»æ¨¡å¼
        
        if no_attack:
            print("âš ï¸  æ— æ”»å‡»æ¨¡å¼ï¼šæ‰€æœ‰æ”»å‡»å·²ç¦ç”¨ï¼Œç›´æ¥è¿”å›åŸå›¾ï¼ˆç”¨äºç¬¬ä¸€é˜¶æ®µè®­ç»ƒï¼‰")

        # å»¶è¿ŸåŠ è½½é‡æ¨¡å‹ï¼Œé¿å…æ— ä¾èµ–æ—¶æŠ¥é”™
        self._inpaint_pipe = None
        self._ip2p_pipe = None
        self._ddim_unet = None
        self._ddim_scheduler = None
        self._ddim_vae = None  # VAEç¼–ç å™¨/è§£ç å™¨
    
    def jpeg_compression(self, image, quality=None):
        """
        JPEGå‹ç¼©è¿‘ä¼¼ï¼ˆå¯å¾®ï¼‰
        """
        if quality is None:
            quality = random.uniform(30, 95)
        
        # ç®€åŒ–ç‰ˆï¼šä½¿ç”¨å·ç§¯æ¨¡æ‹Ÿå‹ç¼©
        kernel_size = int(100 / quality)
        if kernel_size % 2 == 0:
            kernel_size += 1
        kernel_size = max(3, min(kernel_size, 7))
        
        blurred = F.avg_pool2d(
            image, 
            kernel_size=kernel_size, 
            stride=1, 
            padding=kernel_size//2
        )
        return blurred
    
    def affine_transform(self, image):
        """
        ä»¿å°„å˜æ¢ï¼ˆæ—‹è½¬ã€ç¼©æ”¾ï¼‰
        """
        B, C, H, W = image.shape
        
        # éšæœºç”Ÿæˆå˜æ¢å‚æ•°
        angle = random.uniform(-15, 15) * 3.14159 / 180
        scale = random.uniform(0.8, 1.2)
        
        # æ„å»ºå˜æ¢çŸ©é˜µ
        cos_a = scale * torch.cos(torch.tensor(angle))
        sin_a = scale * torch.sin(torch.tensor(angle))
        
        theta = torch.tensor([
            [cos_a, -sin_a, 0],
            [sin_a, cos_a, 0]
        ]).unsqueeze(0).repeat(B, 1, 1).to(image.device)
        
        # ç”Ÿæˆé‡‡æ ·ç½‘æ ¼
        grid = F.affine_grid(theta, image.size(), align_corners=False)
        
        # åº”ç”¨å˜æ¢
        transformed = F.grid_sample(image, grid, align_corners=False)
        
        return transformed
    
    def gaussian_noise(self, image):
        """
        é«˜æ–¯å™ªå£°
        """
        noise_level = random.uniform(0.01, 0.05)
        noise = torch.randn_like(image) * noise_level
        return torch.clamp(image + noise, -1, 1)
    
    def gan_style_attack(self, image):
        """
        GANé£æ ¼æ”»å‡»
        æ¨¡æ‹Ÿç”Ÿæˆæ¨¡å‹çš„å›¾åƒé‡ç»˜
        """
        # ç®€åŒ–å®ç°ï¼šé¢œè‰²æ‰°åŠ¨ + çº¹ç†æ¨¡ç³Š
        
        # 1. é¢œè‰²æ‰°åŠ¨
        color_shift = torch.randn(1, 3, 1, 1).to(image.device) * 0.1
        image_shifted = image + color_shift
        
        # 2. çº¹ç†æ¨¡ç³Š
        image_blurred = F.avg_pool2d(
            image_shifted, 
            kernel_size=3, 
            stride=1, 
            padding=1
        )
        
        # 3. æ··åˆ
        alpha = random.uniform(0.3, 0.7)
        return alpha * image_shifted + (1 - alpha) * image_blurred
    
    def diffusion_attack(self, image):
        """
        æ‰©æ•£æ¨¡å‹æ”»å‡»
        æ¨¡æ‹ŸåŠ å™ª-å»å™ªè¿‡ç¨‹
        """
        # 1. åŠ å™ªï¼ˆæ¨¡æ‹Ÿå‰å‘è¿‡ç¨‹ï¼‰
        noise_level = random.uniform(0.1, 0.3)
        noise = torch.randn_like(image) * noise_level
        noisy_image = image + noise
        
        # 2. å»å™ªï¼ˆç®€åŒ–çš„åå‘è¿‡ç¨‹ï¼‰
        denoised = F.conv2d(
            noisy_image,
            # depthwise 3x3 blur peré€šé“ï¼Œé¿å…groupsç»´åº¦ä¸åŒ¹é…
            torch.ones(3, 1, 5, 5, device=image.device) / 25,
            padding=2,
            groups=3
        )
        
        return torch.clamp(denoised, -1, 1)

    def _load_ddim_model(self, device):
        """
        åŠ è½½é¢„è®­ç»ƒçš„DDIMæ¨¡å‹ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
        
        å®æ–½æ–¹å¼è¦æ±‚ï¼šä½¿ç”¨é¢„è®­ç»ƒä¸”å‚æ•°å†»ç»“çš„å™ªå£°é¢„æµ‹ç½‘ç»œÎµ_Î¸
        """
        if self._ddim_unet is None:
            try:
                from diffusers import UNet2DConditionModel, DDIMScheduler, AutoencoderKL
                from transformers import CLIPTextModel, CLIPTokenizer
                import os
                
                # åŠ è½½é¢„è®­ç»ƒçš„Stable Diffusionæ¨¡å‹ç»„ä»¶
                model_id = "runwayml/stable-diffusion-v1-5"
                
                # å°è¯•ä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼ˆé¿å…ç½‘ç»œè¯·æ±‚ï¼‰
                cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
                local_path = None
                
                # æ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦æœ‰æ¨¡å‹
                if os.path.exists(cache_dir):
                    # æŸ¥æ‰¾æ¨¡å‹å¿«ç…§ç›®å½•
                    model_cache = os.path.join(cache_dir, "models--runwayml--stable-diffusion-v1-5", "snapshots")
                    if os.path.exists(model_cache):
                        snapshots = [d for d in os.listdir(model_cache) if os.path.isdir(os.path.join(model_cache, d))]
                        if snapshots:
                            local_path = os.path.join(model_cache, snapshots[0])
                            print(f"ğŸ“‚ ä½¿ç”¨æœ¬åœ°ç¼“å­˜æ¨¡å‹: {local_path}")
                
                # Mac M4ä¼˜åŒ–ï¼šå¤§å‹æ¨¡å‹æ”¾åˆ°CPUä»¥é¿å…MPSå†…å­˜ä¸è¶³
                # MPSè®¾å¤‡æ£€æµ‹
                use_cpu_for_large_models = (device.type == "mps")
                ddim_device = "cpu" if use_cpu_for_large_models else device
                if use_cpu_for_large_models:
                    print("ğŸ’¡ Mac M4æ£€æµ‹ï¼šå°†DDIMæ¨¡å‹æ”¾åˆ°CPUä»¥é¿å…å†…å­˜ä¸è¶³ï¼ˆé€Ÿåº¦è¾ƒæ…¢ä½†ç¨³å®šï¼‰")
                
                # åŠ è½½VAEï¼ˆç”¨äºRGB <-> Latentè½¬æ¢ï¼‰
                # æ³¨æ„ï¼šVAEå¯èƒ½æ²¡æœ‰ä¸‹è½½ï¼Œå¦‚æœä¸‹è½½å¤±è´¥åˆ™ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
                vae_loaded = False
                if local_path and os.path.exists(os.path.join(local_path, "vae")):
                    try:
                        self._ddim_vae = AutoencoderKL.from_pretrained(
                            local_path,
                            subfolder="vae",
                            local_files_only=True
                        ).to(ddim_device)  # ä½¿ç”¨CPUæˆ–MPS
                        vae_loaded = True
                    except Exception as e:
                        print(f"âš ï¸ VAEæœ¬åœ°åŠ è½½å¤±è´¥: {e}")
                
                if not vae_loaded:
                    print("âš ï¸ VAEæœªåœ¨æœ¬åœ°ç¼“å­˜ä¸­æ‰¾åˆ°ï¼Œå°è¯•ä»ç½‘ç»œä¸‹è½½...")
                    try:
                        self._ddim_vae = AutoencoderKL.from_pretrained(
                            model_id,
                            subfolder="vae"
                        ).to(ddim_device)  # ä½¿ç”¨CPUæˆ–MPS
                        vae_loaded = True
                    except Exception as e:
                        print(f"âš ï¸ VAEä¸‹è½½å¤±è´¥: {e}")
                        print("âš ï¸ å°†ä½¿ç”¨ç®€åŒ–ç‰ˆDDIMæ”»å‡»ï¼ˆä¸ä½¿ç”¨VAEï¼Œç›´æ¥åœ¨RGBç©ºé—´æ“ä½œï¼‰")
                        self._ddim_vae = None
                
                if vae_loaded:
                    for param in self._ddim_vae.parameters():
                        param.requires_grad = False
                    self._ddim_vae.eval()
                
                # åŠ è½½UNetï¼ˆå™ªå£°é¢„æµ‹ç½‘ç»œÎµ_Î¸ï¼‰
                if local_path and os.path.exists(os.path.join(local_path, "unet")):
                    # ä½¿ç”¨æœ¬åœ°è·¯å¾„ï¼Œè®¾ç½®local_files_only=Trueé¿å…ç½‘ç»œè¯·æ±‚
                    self._ddim_unet = UNet2DConditionModel.from_pretrained(
                        local_path,
                        subfolder="unet",
                        local_files_only=True
                    ).to(ddim_device)  # ä½¿ç”¨CPUæˆ–MPS
                else:
                    # å¦‚æœæœ¬åœ°æ²¡æœ‰ï¼Œå°è¯•ä»ç½‘ç»œä¸‹è½½ï¼ˆå¯èƒ½å¤±è´¥ï¼‰
                    self._ddim_unet = UNet2DConditionModel.from_pretrained(
                        model_id, 
                        subfolder="unet"
                    ).to(ddim_device)  # ä½¿ç”¨CPUæˆ–MPS
                # å†»ç»“å‚æ•°ï¼ˆå®æ–½æ–¹å¼è¦æ±‚ï¼‰
                for param in self._ddim_unet.parameters():
                    param.requires_grad = False
                self._ddim_unet.eval()
                
                # åŠ è½½DDIMè°ƒåº¦å™¨ï¼ˆç”¨äºè®¡ç®—Î±Ì„_tï¼‰
                if local_path and os.path.exists(os.path.join(local_path, "scheduler")):
                    self._ddim_scheduler = DDIMScheduler.from_pretrained(
                        local_path,
                        subfolder="scheduler",
                        local_files_only=True
                    )
                else:
                    self._ddim_scheduler = DDIMScheduler.from_pretrained(
                        model_id,
                        subfolder="scheduler"
                    )
                
                # åŠ è½½æ–‡æœ¬ç¼–ç å™¨ï¼ˆç”¨äºæ¡ä»¶ç”Ÿæˆï¼Œä½†å•æ­¥å»å™ªå¯ä»¥ä¸ç”¨ï¼‰
                if local_path and os.path.exists(os.path.join(local_path, "tokenizer")):
                    self._ddim_tokenizer = CLIPTokenizer.from_pretrained(
                        local_path,
                        subfolder="tokenizer",
                        local_files_only=True
                    )
                    self._ddim_text_encoder = CLIPTextModel.from_pretrained(
                        local_path,
                        subfolder="text_encoder",
                        local_files_only=True
                    ).to(ddim_device)  # ä½¿ç”¨CPUæˆ–MPS
                else:
                    self._ddim_tokenizer = CLIPTokenizer.from_pretrained(
                        model_id,
                        subfolder="tokenizer"
                    )
                    self._ddim_text_encoder = CLIPTextModel.from_pretrained(
                        model_id,
                        subfolder="text_encoder"
                    ).to(ddim_device)  # ä½¿ç”¨CPUæˆ–MPS
                for param in self._ddim_text_encoder.parameters():
                    param.requires_grad = False
                self._ddim_text_encoder.eval()
                
                print("âœ… DDIMæ¨¡å‹åŠ è½½æˆåŠŸï¼ˆåŒ…å«VAEï¼‰")
            except ImportError:
                print("âš ï¸ diffusersåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€åŒ–ç‰ˆDDIMæ”»å‡»")
                self.use_ddim = False
            except Exception as e:
                print(f"âš ï¸ DDIMæ¨¡å‹åŠ è½½å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨ç®€åŒ–ç‰ˆDDIMæ”»å‡»")
                self.use_ddim = False
    
    def ddim_attack(self, image):
        """
        å®æ–½æ–¹å¼ï¼ˆ3ï¼‰ï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„å•æ­¥å»å™ªæ”»å‡»
        
        ä½¿ç”¨é¢„è®­ç»ƒä¸”å‚æ•°å†»ç»“çš„å™ªå£°é¢„æµ‹ç½‘ç»œÎµ_Î¸ï¼Œæ ¹æ®DDIMå…¬å¼è®¡ç®—ï¼š
        x_t = I_w
        x_{0_pred} = (x_t - âˆš(1-Î±Ì„_t) â€¢ Îµ_Î¸(x_t, t)) / âˆš(Î±Ì„_t)
        
        Args:
            image: [B, 3, H, W] å¸¦æ°´å°å›¾åƒI_wï¼ŒèŒƒå›´[-1, 1]
        
        Returns:
            x_0_pred: [B, 3, H, W] æ”»å‡»åçš„å›¾åƒï¼ŒèŒƒå›´[-1, 1]
        """
        device = image.device
        B, C, H, W = image.shape
        
        # å¦‚æœæœªå¯ç”¨DDIMæˆ–åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
        if not self.use_ddim:
            return self.pseudo_ddim_denoise(image)
        
        # å»¶è¿ŸåŠ è½½æ¨¡å‹
        self._load_ddim_model(device)
        
        if self._ddim_unet is None:
            return self.pseudo_ddim_denoise(image)
        
        # å¦‚æœVAEæœªåŠ è½½ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬ï¼ˆç›´æ¥åœ¨RGBç©ºé—´æ“ä½œï¼Œä¸ç¬¦åˆæ ‡å‡†ä½†å¯ç”¨ï¼‰
        if self._ddim_vae is None:
            print("âš ï¸ VAEæœªåŠ è½½ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆDDIMï¼ˆRGBç©ºé—´è¿‘ä¼¼ï¼‰")
            return self.pseudo_ddim_denoise(image)
        
        # å°†å›¾åƒä»[-1, 1]è½¬æ¢åˆ°[0, 1]ï¼ˆStable Diffusionçš„è¾“å…¥èŒƒå›´ï¼‰
        image_01 = (image + 1.0) / 2.0
        
        # å°†å›¾åƒresizeåˆ°512x512ï¼ˆStable Diffusionçš„æ ‡å‡†è¾“å…¥å°ºå¯¸ï¼‰
        if H != 512 or W != 512:
            image_512 = F.interpolate(image_01, size=(512, 512), mode='bilinear', align_corners=False)
        else:
            image_512 = image_01
        
        # ä½¿ç”¨VAEç¼–ç å™¨å°†RGBå›¾åƒè½¬æ¢ä¸ºlatent spaceï¼ˆ4é€šé“ï¼‰
        # æ³¨æ„ï¼šå¦‚æœVAEåœ¨CPUä¸Šï¼Œéœ€è¦å°†å›¾åƒç§»åˆ°CPU
        vae_device = next(self._ddim_vae.parameters()).device
        image_512_vae = image_512.to(vae_device) if image_512.device != vae_device else image_512
        
        with torch.no_grad():
            # VAEç¼–ç ï¼šRGB [B,3,512,512] -> Latent [B,4,64,64]
            latent = self._ddim_vae.encode(image_512_vae).latent_dist.sample()
            # ç¼©æ”¾å› å­ï¼ˆStable Diffusionæ ‡å‡†ï¼‰
            latent = latent * self._ddim_vae.config.scaling_factor
        
        # å°†latentç§»å›åŸè®¾å¤‡ï¼ˆå¦‚æœVAEåœ¨CPUä¸Šï¼‰
        latent = latent.to(device)
        
        # éšæœºé€‰æ‹©æ—¶é—´æ­¥tï¼ˆå®æ–½æ–¹å¼è¦æ±‚ï¼‰
        # ä½¿ç”¨è¾ƒå¤§çš„æ—¶é—´æ­¥ä»¥æ¨¡æ‹Ÿæ›´å¼ºçš„æ”»å‡»
        t = torch.randint(
            low=int(0.3 * self._ddim_scheduler.config.num_train_timesteps),
            high=int(0.7 * self._ddim_scheduler.config.num_train_timesteps),
            size=(B,),
            device=device
        )
        
        # è®¡ç®—Î±Ì„_tï¼ˆç´¯ç§¯å™ªå£°è°ƒåº¦ç³»æ•°ï¼‰
        alphas_cumprod = self._ddim_scheduler.alphas_cumprod.to(device)
        alpha_bar_t = alphas_cumprod[t].view(B, 1, 1, 1)
        
        # å‡†å¤‡è¾“å…¥ï¼šå°†latentè§†ä¸ºx_tï¼ˆåŠ å™ªåçš„latentï¼‰
        # ä¸ºäº†æ¨¡æ‹Ÿæ”»å‡»ï¼Œæˆ‘ä»¬å‡è®¾latentå·²ç»æ˜¯æŸä¸ªæ—¶é—´æ­¥çš„åŠ å™ªlatent
        x_t_latent = latent
        
        # å‡†å¤‡æ–‡æœ¬æ¡ä»¶ï¼ˆä½¿ç”¨ç©ºæç¤ºè¯ï¼Œå› ä¸ºå•æ­¥å»å™ªä¸éœ€è¦å¼ºæ¡ä»¶ï¼‰
        # æ³¨æ„ï¼štext_encoderå¯èƒ½åœ¨CPUä¸Šï¼Œéœ€è¦ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®è®¾å¤‡
        text_encoder_device = next(self._ddim_text_encoder.parameters()).device
        
        prompt = [""] * B
        text_inputs = self._ddim_tokenizer(
            prompt,
            padding="max_length",
            max_length=self._ddim_tokenizer.model_max_length,
            truncation=True,
            return_tensors="pt"
        )
        # å°†è¾“å…¥ç§»åˆ°text_encoderæ‰€åœ¨çš„è®¾å¤‡
        text_inputs = {k: v.to(text_encoder_device) for k, v in text_inputs.items()}
        
        with torch.no_grad():
            # è·å–æ–‡æœ¬ç¼–ç 
            text_embeddings = self._ddim_text_encoder(text_inputs['input_ids'])[0]
        
        # å°†text_embeddingsç§»å›åŸè®¾å¤‡ï¼ˆç”¨äºUNetï¼‰
        text_embeddings = text_embeddings.to(device)
        
        # ä½¿ç”¨å™ªå£°é¢„æµ‹ç½‘ç»œÎµ_Î¸é¢„æµ‹å™ªå£°ï¼ˆå®æ–½æ–¹å¼è¦æ±‚ï¼‰
        # æ³¨æ„ï¼šå¦‚æœUNetåœ¨CPUä¸Šï¼Œéœ€è¦å°†è¾“å…¥ç§»åˆ°CPU
        unet_device = next(self._ddim_unet.parameters()).device
        x_t_latent_unet = x_t_latent.to(unet_device) if x_t_latent.device != unet_device else x_t_latent
        text_embeddings_unet = text_embeddings.to(unet_device) if text_embeddings.device != unet_device else text_embeddings
        
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å¯ç”¨æ¢¯åº¦ä»¥æ”¯æŒç«¯åˆ°ç«¯è®­ç»ƒ
        noise_pred = self._ddim_unet(
            x_t_latent_unet,
            t.to(unet_device),
            encoder_hidden_states=text_embeddings_unet
        ).sample
        
        # å°†ç»“æœç§»å›åŸè®¾å¤‡
        noise_pred = noise_pred.to(device)
        
        # æ ¹æ®DDIMå…¬å¼è®¡ç®—x_0_predï¼ˆå®æ–½æ–¹å¼å…¬å¼ï¼‰
        # x_{0_pred} = (x_t - âˆš(1-Î±Ì„_t) â€¢ Îµ_Î¸(x_t, t)) / âˆš(Î±Ì„_t)
        sqrt_alpha_bar_t = torch.sqrt(alpha_bar_t)
        sqrt_one_minus_alpha_bar_t = torch.sqrt(1.0 - alpha_bar_t)
        
        x_0_pred_latent = (x_t_latent - sqrt_one_minus_alpha_bar_t * noise_pred) / sqrt_alpha_bar_t
        
        # ä½¿ç”¨VAEè§£ç å™¨å°†latentè½¬æ¢å›RGBå›¾åƒ
        # æ³¨æ„ï¼šå¦‚æœVAEåœ¨CPUä¸Šï¼Œéœ€è¦å°†latentç§»åˆ°CPU
        x_0_pred_latent_vae = x_0_pred_latent.to(vae_device) if x_0_pred_latent.device != vae_device else x_0_pred_latent
        
        with torch.no_grad():
            # åç¼©æ”¾
            x_0_pred_latent_vae = x_0_pred_latent_vae / self._ddim_vae.config.scaling_factor
            # VAEè§£ç ï¼šLatent [B,4,64,64] -> RGB [B,3,512,512]
            x_0_pred = self._ddim_vae.decode(x_0_pred_latent_vae).sample
        
        # å°†ç»“æœç§»å›åŸè®¾å¤‡
        x_0_pred = x_0_pred.to(device)
        
        # å°†ç»“æœresizeå›åŸå§‹å°ºå¯¸
        if H != 512 or W != 512:
            x_0_pred = F.interpolate(x_0_pred, size=(H, W), mode='bilinear', align_corners=False)
        
        # å°†å›¾åƒä»[0, 1]è½¬æ¢å›[-1, 1]
        x_0_pred = x_0_pred * 2.0 - 1.0
        
        return torch.clamp(x_0_pred, -1, 1)

    def pseudo_ddim_denoise(self, image):
        """
        è½»é‡â€œDDIMå»å™ªâ€å ä½ï¼šå…ˆåŠ å™ªï¼Œå†è¿›è¡Œå¯å¾®å¹³æ»‘ï¼Œè¿‘ä¼¼é‡ç»˜/å»å™ªæ•ˆæœ
        """
        # æ¨¡æ‹ŸDDIMå•æ­¥å»å™ªå…¬å¼ï¼šx_{0_pred} = (x_t - âˆš(1-Î±Ì„_t) â€¢ Îµ_Î¸(x_t, t)) / âˆš(Î±Ì„_t)
        B, C, H, W = image.shape
        device = image.device
        dtype = image.dtype
        
        # æ¨¡æ‹Ÿæ—¶é—´æ­¥tå’ŒÎ±Ì„_tï¼ˆDDIMè°ƒåº¦ï¼‰
        # éšæœºé€‰æ‹©"æ—¶é—´æ­¥"ï¼ˆæ¨¡æ‹Ÿ30%-70%èŒƒå›´ï¼Œå¯¹åº”çœŸæ­£çš„DDIMï¼‰
        t_ratio = random.uniform(0.3, 0.7)
        alpha_bar_t = 1.0 - t_ratio  # ç®€åŒ–çš„Î±Ì„_t
        sqrt_alpha_bar_t = torch.sqrt(torch.tensor(alpha_bar_t, device=device, dtype=dtype))
        sqrt_one_minus_alpha_bar_t = torch.sqrt(torch.tensor(1.0 - alpha_bar_t, device=device, dtype=dtype))
        
        # å°†I_wè§†ä¸ºx_tï¼ˆåŠ å™ªåçš„å›¾åƒï¼‰
        x_t = image
        
        # æ¨¡æ‹Ÿå™ªå£°é¢„æµ‹ç½‘ç»œÎµ_Î¸(x_t, t) - ä½¿ç”¨è½»é‡å·ç§¯ç½‘ç»œ
        # åˆå§‹åŒ–å›ºå®šçš„éšæœºæƒé‡ï¼ˆè½»é‡ï¼Œä¸éœ€è¦è®­ç»ƒï¼‰
        if not hasattr(self, '_pseudo_ddim_conv1'):
            torch.manual_seed(42)  # å›ºå®šéšæœºç§å­ç¡®ä¿å¯é‡å¤
            self._pseudo_ddim_conv1 = torch.randn(32, 3, 3, 3, device=device, dtype=dtype, requires_grad=False) * 0.1
            self._pseudo_ddim_conv2 = torch.randn(64, 32, 3, 3, device=device, dtype=dtype, requires_grad=False) * 0.1
            self._pseudo_ddim_conv3 = torch.randn(3, 32+64, 3, 3, device=device, dtype=dtype, requires_grad=False) * 0.1
        
        # ç‰¹å¾æå–ï¼ˆæ¨¡æ‹ŸUNetç¼–ç å™¨ï¼‰
        feat1 = F.conv2d(x_t, self._pseudo_ddim_conv1, padding=1)
        feat1 = F.relu(feat1)
        
        # ä¸‹é‡‡æ ·ï¼ˆæ¨¡æ‹ŸUNetç“¶é¢ˆå±‚ï¼‰
        feat2 = F.avg_pool2d(feat1, 2)
        feat2 = F.conv2d(feat2, self._pseudo_ddim_conv2, padding=1)
        feat2 = F.relu(feat2)
        
        # ä¸Šé‡‡æ ·å¹¶é¢„æµ‹å™ªå£°ï¼ˆæ¨¡æ‹ŸUNetè§£ç å™¨ï¼‰
        feat2_up = F.interpolate(feat2, size=(H, W), mode='bilinear', align_corners=False)
        noise_pred = F.conv2d(
            torch.cat([feat1, feat2_up], dim=1),
            self._pseudo_ddim_conv3,
            padding=1
        )
        
        # å½’ä¸€åŒ–å™ªå£°é¢„æµ‹
        noise_pred = noise_pred * 0.15  # ç¼©æ”¾å› å­
        
        # æ ¹æ®DDIMå…¬å¼è®¡ç®—x_0_predï¼ˆå®æ–½æ–¹å¼å…¬å¼ï¼‰
        x_0_pred = (x_t - sqrt_one_minus_alpha_bar_t * noise_pred) / sqrt_alpha_bar_t
        
        return torch.clamp(x_0_pred, -1, 1)

    def inpaint_attack(self, image):
        """
        è½»é‡å ä½ï¼šè‹¥æ—  diffusers ä¾èµ–æˆ–æœªå¯ç”¨ï¼Œåˆ™ç›´æ¥è¿”å›
        """
        if not self.use_inpaint:
            return image
        try:
            if self._inpaint_pipe is None:
                from diffusers import StableDiffusionInpaintPipeline
                self._inpaint_pipe = StableDiffusionInpaintPipeline.from_pretrained(
                    "runwayml/stable-diffusion-inpainting"
                ).to(image.device)
                self._inpaint_pipe.set_progress_bar_config(disable=True)
            B, C, H, W = image.shape
            # æ„é€ éšæœºé®ç½©ï¼ˆ20%-40%åŒºåŸŸï¼‰
            mask = torch.zeros_like(image[:, :1])
            ratio = random.uniform(0.2, 0.4)
            h = int(H * ratio)
            w = int(W * ratio)
            top = random.randint(0, H - h)
            left = random.randint(0, W - w)
            mask[:, :, top:top+h, left:left+w] = 1.0
            # è½¬ä¸º PIL æ‰¹å¤„ç†è¾ƒé‡ï¼Œè¿™é‡Œä»…åœ¨å¼€å¯æ—¶æ‰è¿è¡Œ
            imgs = (image.clamp(-1, 1) * 127.5 + 127.5).byte().cpu()
            masks = (mask * 255).byte().cpu()
            pil_imgs = [transforms.ToPILImage()(imgs[i]) for i in range(B)]
            pil_masks = [transforms.ToPILImage()(masks[i]) for i in range(B)]
            results = []
            for img_pil, m_pil in zip(pil_imgs, pil_masks):
                out = self._inpaint_pipe(prompt="restore the image", image=img_pil, mask_image=m_pil).images[0]
                tensor = transforms.ToTensor()(out).to(image.device) * 2 - 1
                results.append(tensor)
            return torch.stack(results, dim=0)
        except Exception:
            return image

    def ip2p_attack(self, image):
        """
        è½»é‡å ä½ï¼šInstructPix2Pix é£æ ¼ç¼–è¾‘ï¼ˆéœ€ diffusersï¼‰ï¼Œé»˜è®¤å…³é—­
        """
        if not self.use_ip2p:
            return image
        try:
            if self._ip2p_pipe is None:
                from diffusers import StableDiffusionInstructPix2PixPipeline
                self._ip2p_pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(
                    "timbrooks/instruct-pix2pix"
                ).to(image.device)
                self._ip2p_pipe.set_progress_bar_config(disable=True)
            prompt = random.choice([
                "make it oil painting style",
                "convert to watercolor",
                "add cinematic lighting",
                "make it sketch style",
            ])
            imgs = (image.clamp(-1, 1) * 127.5 + 127.5).byte().cpu()
            pil_imgs = [transforms.ToPILImage()(imgs[i]) for i in range(imgs.size(0))]
            results = []
            for img_pil in pil_imgs:
                out = self._ip2p_pipe(prompt=prompt, image=img_pil, num_inference_steps=4, guidance_scale=1.5).images[0]
                tensor = transforms.ToTensor()(out).to(image.device) * 2 - 1
                results.append(tensor)
            return torch.stack(results, dim=0)
        except Exception:
            return image
    
    def forward(self, image):
        """
        éšæœºé€‰æ‹©æ”»å‡»
        
        Args:
            image: [B, 3, H, W]
        
        Returns:
            attacked_image: [B, 3, H, W]
        """
        # æ— æ”»å‡»æ¨¡å¼ï¼šç›´æ¥è¿”å›åŸå›¾ï¼ˆç”¨äºç¬¬ä¸€é˜¶æ®µè®­ç»ƒï¼‰
        if self.no_attack:
            return image
        
        # æ”»å‡»æ± 
        attacks = [
            self.jpeg_compression,
            self.affine_transform,
            self.gaussian_noise,
            self.gan_style_attack,
            self.diffusion_attack
        ]
        # å®æ–½æ–¹å¼è¦æ±‚ï¼šä½¿ç”¨çœŸæ­£çš„DDIMæ”»å‡»
        if self.use_ddim:
            attacks.append(self.ddim_attack)
        elif self.use_light_aigc:
            # å¦‚æœDDIMä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
            attacks.append(self.pseudo_ddim_denoise)
        if self.use_inpaint:
            attacks.append(self.inpaint_attack)
        if self.use_ip2p:
            attacks.append(self.ip2p_attack)

        # è½®è¯¢ä¿è¯è¦†ç›–ï¼Œå†éšæœºè¡¥å……ç¬¬äºŒä¸ª
        first_attack = attacks[self._attack_index % len(attacks)]
        self._attack_index += 1

        num_attacks = random.choice([1, 2])
        if num_attacks == 2:
            remaining = [a for a in attacks if a is not first_attack]
            second_attack = random.choice(remaining)
            selected_attacks = [first_attack, second_attack]
        else:
            selected_attacks = [first_attack]
        
        # ä¾æ¬¡åº”ç”¨æ”»å‡»
        attacked_image = image
        for attack in selected_attacks:
            attacked_image = attack(attacked_image)
        
        return attacked_image


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    
    attack_module = HeterogeneousAttack().to(device)
    
    # æµ‹è¯•è¾“å…¥
    image = torch.randn(4, 3, 256, 256).to(device)
    
    # æµ‹è¯•æ”»å‡»
    attacked = attack_module(image)
    
    print(f"âœ… æ”»å‡»æ¨¡å—æµ‹è¯•é€šè¿‡")
    print(f"   è¾“å…¥: {image.shape}")
    print(f"   è¾“å‡º: {attacked.shape}")
    print(f"   è®¾å¤‡: {device}")